{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the 'reuters' corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "lo8i8SaHqaIe"
   },
   "outputs": [],
   "source": [
    "# Necessary installations \n",
    "#!pip install nltk\n",
    "#!pip install numpy\n",
    "\n",
    "# Start by importing the necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "\n",
    "# Additionally used libraries\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kBr3pLmryq8",
    "outputId": "bd9b60d4-69d9-4bda-eb5b-4d54565d4e7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/lisa/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lisa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the reuters corpus is downloaded\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "4zJsZVNxrz5q"
   },
   "outputs": [],
   "source": [
    "# Load the reuters corpus\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    \"\"\"\n",
    "    Preprocess the raw text of a corpus. Namely remove numbers, lowercase, remove puncutation, tokenize.\n",
    "    Parameters:\n",
    "        corpus (str): the corpus of text to be preprocessed\n",
    "    Returns:\n",
    "        corpus (list): the preprocessed corpus\n",
    "    \"\"\"\n",
    "    # remove numbers\n",
    "    corpus = re.sub(r'\\d+', ' ', corpus)\n",
    "    # lowercase everything\n",
    "    corpus = corpus.lower() \n",
    "    # remove punctuation \n",
    "    corpus = \"\".join([char for char in corpus if char not in string.punctuation])\n",
    "    # tokenize \n",
    "    corpus = nltk.word_tokenize(corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'usjapan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'us', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asias', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'farreaching']\n"
     ]
    }
   ],
   "source": [
    "prep_reuters = preprocessing(reuters.raw()) # .raw() to extract text from corpus \n",
    "\n",
    "print(prep_reuters[:30])\n",
    "# usjapan from u.s.-japan ???????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. N-grams & 4. their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(corpus, n:int):\n",
    "    \"\"\"\n",
    "    Creates ngrams of a preprocessed corpus.\n",
    "    Parameters:\n",
    "        corpus (list): a preprocessed (lowercase, no punctuation, no numbers, tokenized) corpus\n",
    "        n (int): the length of each ngram\n",
    "    Returns:\n",
    "        ngrams (zip): the ngrams of the corpus\n",
    "    \"\"\"\n",
    "    ngrams = nltk.ngrams(corpus, n) \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngrams(ngrams):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each ngram and displays them. ???????????????????????????\n",
    "    Parameters:\n",
    "        ngrams (zip): the ngrams to count the frequencies of\n",
    "    Returns: \n",
    "        freq_ngrams (FreqDist): each ngram and its frequency\n",
    "    \"\"\"\n",
    "    freq_ngrams = nltk.FreqDist(ngrams)\n",
    "    # display not showing, however showing if called afterwards see below?????????????????????\n",
    "    # print(freq_ngrams.items()) \n",
    "    # print(freq_ngrams) \n",
    "    # @ToDO: display ngrams!!!!!!!!!!!\n",
    "    return freq_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFor testing:\\n\\nprint(len(list(freq_ngrams_reuters.items())[-3:]))\\nprint(list(freq_ngrams_reuters.items())[-3+1:])\\nprint(list(freq_ngrams_reuters)[-3+1:])\\nposition = np.argmax([i[1] for i in list(freq_ngrams_reuters.items())[-3+1:]])\\nprint(position)\\n#tuppel = (1,2)\\n#tuppel[1]\\npreprocessing('hi du 55 gestresstes panterleun')[-2:]\\n\""
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_reuters = ngrams(prep_reuters, 3)\n",
    "\n",
    "freq_ngrams_reuters = freq_ngrams(ngrams_reuters)\n",
    "freq_ngrams_reuters #?????????????????????? \n",
    "\n",
    "\"\"\"\n",
    "For testing:\n",
    "\n",
    "print(len(list(freq_ngrams_reuters.items())[-3:]))\n",
    "print(list(freq_ngrams_reuters.items())[-3+1:])\n",
    "print(list(freq_ngrams_reuters)[-3+1:])\n",
    "position = np.argmax([i[1] for i in list(freq_ngrams_reuters.items())[-3+1:]])\n",
    "print(position)\n",
    "#tuppel = (1,2)\n",
    "#tuppel[1]\n",
    "preprocessing('hi du 55 gestresstes panterleun')[-2:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word_model(input_sentence, freq_ngrams): \n",
    "    \"\"\"\n",
    "    Predict the next word of an input sentence. \n",
    "    Parameters:\n",
    "        input_sentence (string): the sentence to predict a new word for\n",
    "        freq_ngrams (FreqDist): the ngrams and their frequencies to base the prediction on\n",
    "    Returns:\n",
    "        next_word (string): the next word or a no-next-word message\n",
    "    \"\"\"\n",
    "    # preprocess input sequence\n",
    "    prep_input_sentence = preprocessing(input_sentence) \n",
    "    # change type of the freq_ngrams class object to list\n",
    "    freq_ngrams = list(freq_ngrams.items())\n",
    "    # extract the last n-1 words from the sentence, \n",
    "    # where n is the length of the ngrams\n",
    "    len_ngrams = len(freq_ngrams[0][0])\n",
    "    last_n_words = prep_input_sentence[-len_ngrams+1:]\n",
    "    # collect all possible next words and their frequencies in a list\n",
    "    possible_next_words = []\n",
    "    for ngram in freq_ngrams:\n",
    "        i = 0\n",
    "        for word in last_n_words: \n",
    "            if ngram[0][i] == word:\n",
    "                i += 1\n",
    "                if i == len_ngrams-1:\n",
    "                    possible_next_words.append(ngram)\n",
    "            else:\n",
    "                break\n",
    "    # choose the next word according to the highest frequency\n",
    "    # make sure there is at least one possible word\n",
    "    if possible_next_words == []:\n",
    "        next_word = 'No next word based on this corpus'\n",
    "    else: \n",
    "        position = np.argmax([i[1] for i in possible_next_words])\n",
    "        next_word = possible_next_words[position][0][len_ngrams-1]\n",
    "    \n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'hi du 55 ... panterleun automotive technologies of the ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_word_model(input_sentence, freq_ngrams_reuters)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
